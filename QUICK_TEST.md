# Quick Test Guide

## Testing the Fixed LLM Integration

### Current State (Template Mode)
1. **Open the app**: Click the preview button to open http://localhost:5177
2. **Navigate**: Click on any fortune-telling method (e.g., "å…«å­—å‘½ç†" or "å¡”ç½—å åœ")
3. **Ask a question**: Type any question like "å¥åº·" or "å·¥ä½œè¿åŠ¿å¦‚ä½•"
4. **Click "å¼€å§‹"**: The app will process and show results
5. **Check the result**: You should see:
   - âš ï¸ "æ™ºèƒ½æ¨¡æ¿æ¨¡å¼" indicator 
   - ğŸ”Œ "ç¦»çº¿æ¨¡å¼" status in results
   - Contextual, relevant response (not generic)

### What's Fixed
- âœ… **No more 500 errors**: App gracefully handles missing serverless function
- âœ… **Smart fallback**: Generates contextual responses based on question topics
- âœ… **Clear status**: Shows whether AI or template mode is active
- âœ… **Better UX**: Always provides meaningful results

### Console Output
Check browser DevTools Console to see:
```
Starting divination process...
LLM Service configured: false
Serverless function not available in development mode
LLM API not configured, using enhanced fallback
```

### To Enable Real AI (Optional)
1. **Get a DeepSeek API key**: Sign up at https://platform.deepseek.com/
2. **Edit `.env`**: Uncomment and add your API key:
   ```env
   VITE_LLM_API_ENDPOINT=https://api.deepseek.com/v1/chat/completions
   VITE_LLM_API_KEY=sk-your-actual-api-key-here
   VITE_LLM_MODEL=deepseek-chat
   VITE_LLM_TEMPERATURE=0.8
   VITE_LLM_MAX_TOKENS=800
   ```
3. **Server will auto-restart**: Vite detects `.env` changes
4. **Test again**: Should see "AIåˆ†æå·²é…ç½®" and real AI responses

### Expected Behavior
- **Without API**: Smart template responses, clearly marked as template mode
- **With API**: Real AI analysis, marked with "AIåˆ†æ" status
- **API Failure**: Graceful fallback to smart templates with error message

The error `POST http://localhost:5173/api/llm/chat 500` is now handled gracefully and won't prevent the app from working!